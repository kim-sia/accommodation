{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_accommodation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhL6IjvxciXv",
        "outputId": "9ed215f8-30c9-4c22-d730-b2564e3b2441"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.3.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YidfyPu2cs23"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import torch\r\n",
        "\r\n",
        "from transformers import BertTokenizer\r\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\r\n",
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import time\r\n",
        "import datetime"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuE9A72Gcw_b",
        "outputId": "0164f874-bed9-4709-dde8-bac3f8363c95"
      },
      "source": [
        "train = pd.read_csv(open('accommodation_review.csv'), encoding = 'utf-8', sep=',', names = ['place_id', 'review_id', 'date', 'review_point', 'review', 'preprocessed_review'])\r\n",
        "\r\n",
        "print(train.shape)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17811, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "NsGtMpKSg0dQ",
        "outputId": "39feaafe-4a83-49c9-f9a7-cfe246048ede"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>place_id</th>\n",
              "      <th>review_id</th>\n",
              "      <th>date</th>\n",
              "      <th>review_point</th>\n",
              "      <th>review</th>\n",
              "      <th>preprocessed_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9479134</td>\n",
              "      <td>3044249</td>\n",
              "      <td>2021-02-03</td>\n",
              "      <td>1.0</td>\n",
              "      <td>♩♬♩♬입니다.전날 4만원 받았는데, 다음날 5만원..첨에는 4만원 받은적이 없다고...</td>\n",
              "      <td>입니다 전날 만원 받았는데 다음날 만원 첨에는 만원 받은 적이 없다고 우기더니 전날...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15256072</td>\n",
              "      <td>876254</td>\n",
              "      <td>2013-07-12</td>\n",
              "      <td>5.0</td>\n",
              "      <td>여름 휴가때마다 여기서 보낸답니다. 가리왕산자연휴양림 입구가 바로 앞에 있어서 산책...</td>\n",
              "      <td>여름휴가 때마다 여기서 보낸답니다 가리왕산 자연휴양림 입구가 바로 앞에 있어서 산책...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1613530490</td>\n",
              "      <td>1586165</td>\n",
              "      <td>2020-02-29</td>\n",
              "      <td>1.0</td>\n",
              "      <td>오션뷰는 좋네요^^저녁에 근처횟집가서 저녁을먹을까 고민하다가 날씨도 쌀쌀하고 귀찮기...</td>\n",
              "      <td>오션뷰는 좋네요 저녁에 근처 횟집 가서 저녁을 먹을까 고민하다가 날씨도 쌀쌀하고 귀...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>26404600</td>\n",
              "      <td>1864635</td>\n",
              "      <td>2017-12-21</td>\n",
              "      <td>5.0</td>\n",
              "      <td>펜션이 이렇게 좋아도 되나... 싶은... 매우 훌륭합니다.</td>\n",
              "      <td>펜션이 이렇게 좋아도 되나 싶은 매우 훌륭합니다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9662665</td>\n",
              "      <td>2697483</td>\n",
              "      <td>2020-11-19</td>\n",
              "      <td>3.0</td>\n",
              "      <td>방이커요</td>\n",
              "      <td>방이 커요</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     place_id  ...                                preprocessed_review\n",
              "0     9479134  ...  입니다 전날 만원 받았는데 다음날 만원 첨에는 만원 받은 적이 없다고 우기더니 전날...\n",
              "1    15256072  ...  여름휴가 때마다 여기서 보낸답니다 가리왕산 자연휴양림 입구가 바로 앞에 있어서 산책...\n",
              "2  1613530490  ...  오션뷰는 좋네요 저녁에 근처 횟집 가서 저녁을 먹을까 고민하다가 날씨도 쌀쌀하고 귀...\n",
              "3    26404600  ...                         펜션이 이렇게 좋아도 되나 싶은 매우 훌륭합니다\n",
              "4     9662665  ...                                              방이 커요\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgjMzosCDD35"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# **전처리 - 훈련셋**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmTZDnTrhvPD",
        "outputId": "b4383e92-ba0d-4538-c559-be6d334eaf9d"
      },
      "source": [
        "# 리뷰 문장 추출\r\n",
        "sentences = train['preprocessed_review']\r\n",
        "sentences[:10]"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    입니다 전날 만원 받았는데 다음날 만원 첨에는 만원 받은 적이 없다고 우기더니 전날...\n",
              "1    여름휴가 때마다 여기서 보낸답니다 가리왕산 자연휴양림 입구가 바로 앞에 있어서 산책...\n",
              "2    오션뷰는 좋네요 저녁에 근처 횟집 가서 저녁을 먹을까 고민하다가 날씨도 쌀쌀하고 귀...\n",
              "3                           펜션이 이렇게 좋아도 되나 싶은 매우 훌륭합니다\n",
              "4                                                방이 커요\n",
              "5    듀오 청소상태 뜨악 들어가자마자 파리들이 놀고 있고 베개 밑에 뭐가 있었는지 아침에...\n",
              "6    오픈하고 얼마 지나지 않았는지 일단 깨끗해서 너무 좋았어요 인테리어도 그렇고 오랜만...\n",
              "7    강릉 호텔 중 이 가격대에서 강릉역에도 접근성 좋고 강릉시내 많은 맛 집들과도 밀접...\n",
              "8             작년엔 이문세 올해는 이승철 콘서트 매년 광복절마다 시원한 용평에서 힐링\n",
              "9    솔 게스트하우스 양양 서핑 청 시행 청 시행 비치 서핑 다 같은 곳이에요 처음에 예...\n",
              "Name: preprocessed_review, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCUHQwwLh74_",
        "outputId": "cc1c48b6-8700-45cd-85e7-22dc58783340"
      },
      "source": [
        "# BERT의 입력 형식에 맞게 변환\r\n",
        "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\r\n",
        "sentences[:10]"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] 입니다 전날 만원 받았는데 다음날 만원 첨에는 만원 받은 적이 없다고 우기더니 전날 영수증 내미니까 오늘은 특실이래요 전날 방이 더 크고 시설도 더 낳은데 만 원에 사람 만들고 이런 집에 다시는 가고 싶지 않네요 [SEP]',\n",
              " '[CLS] 여름휴가 때마다 여기서 보낸답니다 가리왕산 자연휴양림 입구가 바로 앞에 있어서 산책하기 좋고 앞에 아이들이 물놀이할 수 있는 곳도 있어서 여러모로 좋은 곳이네요 더불어 사장님의 친절함과 맛깔스러운 음식들 정선에 구경할 만한 곳도 잘 가르쳐 주시더라고요 암튼 해마다 찾아도 변함없고 답답한 도시를 벗어나서 힐링할 수 있는 멋진 곳인 것 같아요 [SEP]',\n",
              " '[CLS] 오션뷰는 좋네요 저녁에 근처 횟집 가서 저녁을 먹을까 고민하다가 날씨도 쌀쌀하고 귀찮기도 해서 층 flavor 석식 뷔페를 먹기로 결정하고 투숙객 프로 할인과 추가 시 와인 무제한 이벤트 진행 중이길래 쉽게 결정하고 맛있게 저녁식사를 하려고 하는데 도가니탕에서 검은 털이 나왔네요 직원을 불러 머리카락이 있어요 라고 말하니 확인해보겠다고 그릇을 가져간 후 도가니 담겨있던 그릇을 다시 가져와서는 하는 말 돼지털이네요 도가니에 왜 돼지털이 나오나요 돼지털이 검정 털이라고 되물으니 나중엔 소털이란다 나 참 ㅋㅋㅋ 죄송하다고 하면 끝날 일 절대 우리 실수 아니고 소털이다 그래서 그 털 어쨌냐 다시 보게 달라고 하니 음식 그릇에서 털만 빼고 다시 가져오고는 없단다 모르겠단다 계산하고 가려 하니 투숙객 프로 할인과 와인 무제한은 중복할인이라 안된다 이건 뭐 강릉에 오성급 호텔 맞나 별 하나도 아깝다 글 좀 잘 쓰시지 어디 중복할인 안된다는 글이 있나라고 물으니 계산 원한 분이 위아래로 사람을 쳐다본다 직원 교육 다 시 부탁드리고요 안내문 정확히 써주세요 한글이 어렵나 이게 오성급 호텔의 안내문인가 [SEP]',\n",
              " '[CLS] 펜션이 이렇게 좋아도 되나 싶은 매우 훌륭합니다 [SEP]',\n",
              " '[CLS] 방이 커요 [SEP]',\n",
              " '[CLS] 듀오 청소상태 뜨악 들어가자마자 파리들이 놀고 있고 베개 밑에 뭐가 있었는지 아침에 팔에 뭐가 붙어있어서 떼어내니 빨갛게 붓고 프라이팬 코팅은 다 벗겨져있고 연 박하는데 가격은 하나도 안 깎아주면서 청소는 안 해주고 열받아서 그 이후로 다시는 듀오 이용 안 합니다 년 전쯤 옆에 호텔들이 우르르 생긴 이후로는 서비스가 좋아졌어요 그전에는 배 째라 하였는데 ᄏ 하루에 한 번 물 교체하는 유아풀이 좋고 스위트룸 층에서는 뷰가 끝내줬습니다 새벽에 오징어잡이 배가 보이는데 엄청 밝아요 꼭 커튼 치셔야 앞에 기사식당과 편의점이 있어서 식사 걱정도 없었고 배달 다 되고 알아보니 근처에도 이상 식당도 몇 있네요 가장 저렴한 방도 호텔같이 좋았고 리조트 내 정원도 좋고 리조트가 전체 대리석인지 게다가 통풍도 잘되어 그게 제일 좋았어요 여름에 시원하거든요 h동 더 스카이 런치 뷔페도 좋았습니다 지금은 패키지 예약 시 연방 할인도 됩니다 [SEP]',\n",
              " '[CLS] 오픈하고 얼마 지나지 않았는지 일단 깨끗해서 너무 좋았어요 인테리어도 그렇고 오랜만에 여자친구랑 즐거운 시간 보냈습니다 [SEP]',\n",
              " '[CLS] 강릉 호텔 중 이 가격대에서 강릉역에도 접근성 좋고 강릉시내 많은 맛 집들과도 밀접하고 호텔 내에 수영장도 있고 가장 크고 가장 좋습니다 리뷰에 주변 모텔이나 펜션 같은 숙박업 하시는 사장님들이 말도 안 되는 리뷰 남기시는 분들이 참 많은 거 같네요 무슨 청소가 안 돼있다느니 곰팡이가 심각하다느니 불친절하다느니 모텔이 낫다느니 무슨 두바이 성급 호텔을 바라시는 것도 아니고 다른 호텔들을 가보신 적도 없는 건지 모르겠네요 심지어 호텔방에 들어가 보시기는 하신 걸까요 불친절하다는 사람들의 대부분은 직원들한테 먼저 예의 없이 굴거나 하대하는 꼰대 마인드를 가진 분들이 대부분인 것 같네요 저는 매년 학회를 다니면서 국내 국외 나름 호텔을 다녀보았는데 경험상 직원분들이 막 프로페셔널하시다는 느낌은 없습니다만 그렇다고 불친절하신 건 절대 아닙니다 직원분들은 충분히 본인들이 할 의무를 다하고 계셨고 충분히 친절하게 응대해주셨습니다 제가 도착해서 체크인 시간이 분 정도 남았는데 리뷰에서 조기 체크인을 안 해준다는 둥 불친절하다는 말이 너무 많아서 괜히 안 좋은 일이 생길까 봐 살짝 걱정했지만 직원분께 가서 혹시 죄송합니다만 지금 체크인이 가능할까요 토씨 하나 틀리지 않고라고 말씀드리자 잠시 확인 후 곧바로 조기 체크인을 해주셨습니다 엘리베이터가 몇 대 없어서 불편하긴 합니다만 아니 국내 호텔 중에 어디 호텔이 엘리베이터가 그렇게 많습니까 조식은 두 개가 있는데 위쪽 층에서 바다 뷰를 보면서 싸고 가짓수가 적은 곳이랑 낮은 층에서 조금 비싸고 가짓수가 많은 곳이 있는데 저는 낮은 층에서 먹었습니다 음식은 충분히 맛있었으며 가짓수도 호텔 조식으로 충분히 많았습니다 조식 가지 수가 부족하다는 분들은 도대체 어디 호텔이랑 비교를 하시는 건지 모르겠고 혹시 결혼식 뷔페나 애슐리랑 비교하시는 건 아닌지 모르겠네요 다른 국내 호텔에 비하면 가짓수는 충분히 많습니다 [SEP]',\n",
              " '[CLS] 작년엔 이문세 올해는 이승철 콘서트 매년 광복절마다 시원한 용평에서 힐링 [SEP]',\n",
              " '[CLS] 솔 게스트하우스 양양 서핑 청 시행 청 시행 비치 서핑 다 같은 곳이에요 처음에 예약할 때 다른 곳인가 엄청 헷갈렸는데 결국 다 같은 곳이에요 서핑 스폿은 솔 게스트하우스 바로 앞이고요 숙소는 본 건물 층이 꽉 차면 양양 쪽 숙소로 잡아주더라고요 숙소는 완전 깨끗 양 양쪽 숙소를 가더라도 셔틀버스를 계속 운행해주셔요 차가 없어도 움직이는데 불편함이 없을듯합니다 서핑할 때 필요한 준비물 슈트 안에 입을 속옷 세면용품 서핑 끝나고 갈아입을 여벌옷 속옷 포함 선크림 해변용 슬리퍼 슈트는 전신 슈트이기 때문에 선크림은 얼굴이랑 목 정도에만 발라주면 돼요 시간 정도 서핑한다고 계속 물에 있었는데 많이 안 탔어요 아 그리고 서핑 강의가 끝나면 해질 때까지 슈트와 보드는 계속 이용할 수 있어요 체력왕이 신 분들은 오전으로 신청하시면 꽉 차게 놀 수 있겠네요 친절한 직원분들 내가 놀러 왔구나 느낄 수 있게 해주는 친절함이랍니다 직원분들 덕분에 여행이 더 즐겁게 느껴져요 파티 제공되는 바비큐도 훌륭하고요 중간에 진행되는 레크리에이션도 정말 재밌습니다 시반부터 시까지 아주 신나게 놀 수 있어요 다만 본 건물에 숙소가 있는 분들은 늦게까지 시끄러운 음악소리에 잠들지 못할 수 있답니다 햄버거 햄버거 드시러만 들리는 분들이 꽤 있어요 바로 앞에 바다가 보이니 햄버거만 먹어도 기분짱 저는 개인적으로 바닷가에서 사 먹는 음식은 다 평균 이하라고 생각했는데 여기는 좋았습니다 아무래도 관광지다 보니 가격이 조금 비싸지요 네이버로 주문하면 아메리카노를 서비스로 준답니다 참고하세요 서핑 용품 대여 강의 숙소 바비큐 파티 저녁식사까지 해결되는 바비큐 파티까지 모두 포함해서 인당 만 원이라뇨 정말 최고의 여행이었습니다 돈이 하나도 안 아까워요 다들 놀러 가세요 광고 아님 주의 [SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvqU4LZTh9v8",
        "outputId": "def16a58-d5d8-44d8-c00a-084697c041d7"
      },
      "source": [
        "# 라벨 추출\r\n",
        "labels = train['review_point'].values\r\n",
        "labels"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 5., 1., ..., 5., 5., 5.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EAM6OjyiKmA",
        "outputId": "97f16a12-1b1a-49fd-e32e-e25438532e89"
      },
      "source": [
        "# BERT의 토크나이저로 문장을 토큰으로 분리\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\r\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\r\n",
        "\r\n",
        "print (sentences[0])\r\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] 입니다 전날 만원 받았는데 다음날 만원 첨에는 만원 받은 적이 없다고 우기더니 전날 영수증 내미니까 오늘은 특실이래요 전날 방이 더 크고 시설도 더 낳은데 만 원에 사람 만들고 이런 집에 다시는 가고 싶지 않네요 [SEP]\n",
            "['[CLS]', '입', '##니다', '전', '##날', '만', '##원', '받', '##았', '##는데', '다음', '##날', '만', '##원', '첨', '##에는', '만', '##원', '받은', '적', '##이', '없다', '##고', '우', '##기', '##더', '##니', '전', '##날', '영', '##수', '##증', '내', '##미', '##니', '##까', '오', '##늘', '##은', '특', '##실', '##이', '##래', '##요', '전', '##날', '방', '##이', '더', '크', '##고', '시', '##설', '##도', '더', '낳', '##은', '##데', '만', '원', '##에', '사', '##람', '만', '##들', '##고', '이런', '집', '##에', '다시', '##는', '가', '##고', '싶', '##지', '않', '##네', '##요', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niqSpqB4iQ3Y",
        "outputId": "68b04662-ff82-4bcb-eb8c-fef45197f4aa"
      },
      "source": [
        "# 입력 토큰의 최대 시퀀스 길이\r\n",
        "MAX_LEN = 128\r\n",
        "\r\n",
        "# 토큰을 숫자 인덱스로 변환\r\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\r\n",
        "\r\n",
        "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\r\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\r\n",
        "\r\n",
        "input_ids[0]"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   101,   9645,  48345,   9665,  41919,   9248,  14279,   9322,\n",
              "       119118,  41850,  52292,  41919,   9248,  14279,   9748,  15303,\n",
              "         9248,  14279,  74141,   9664,  10739,  39218,  11664,   9604,\n",
              "        12310,  54141,  25503,   9665,  41919,   9574,  15891, 119230,\n",
              "         8996,  22458,  25503, 118671,   9580, 118762,  10892,   9891,\n",
              "        31503,  10739,  37388,  48549,   9665,  41919,   9328,  10739,\n",
              "         9074,   9834,  11664,   9485,  31928,  12092,   9074,   8995,\n",
              "        10892,  28911,   9248,   9612,  10530,   9405,  61250,   9248,\n",
              "        27023,  11664,  80956,   9711,  10530,  25805,  11018,   8843,\n",
              "        11664,   9495,  12508,   9523,  77884,  48549,    102,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdrttAo7iUEM",
        "outputId": "2e296e2b-c3b5-4405-e7e9-61572e8a1e72"
      },
      "source": [
        "# 어텐션 마스크 초기화\r\n",
        "attention_masks = []\r\n",
        "\r\n",
        "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\r\n",
        "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\r\n",
        "for seq in input_ids:\r\n",
        "    seq_mask = [float(i>0) for i in seq]\r\n",
        "    attention_masks.append(seq_mask)\r\n",
        "\r\n",
        "print(attention_masks[0])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUvkZ10miWOS",
        "outputId": "00d4d455-f5c6-4ec5-881a-de7f04f32527"
      },
      "source": [
        "# 훈련셋과 검증셋으로 분리\r\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\r\n",
        "                                                                                    labels, \r\n",
        "                                                                                    random_state=2018, \r\n",
        "                                                                                    test_size=0.1)\r\n",
        "\r\n",
        "# 어텐션 마스크를 훈련셋과 검증셋으로 분리\r\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \r\n",
        "                                                       input_ids,\r\n",
        "                                                       random_state=2018, \r\n",
        "                                                       test_size=0.1)\r\n",
        "\r\n",
        "# 데이터를 파이토치의 텐서로 변환\r\n",
        "train_inputs = torch.tensor(train_inputs)\r\n",
        "train_labels = torch.tensor(train_labels)\r\n",
        "train_masks = torch.tensor(train_masks)\r\n",
        "validation_inputs = torch.tensor(validation_inputs)\r\n",
        "validation_labels = torch.tensor(validation_labels)\r\n",
        "validation_masks = torch.tensor(validation_masks)\t\t\t\t\r\n",
        "\r\n",
        "print(train_inputs[0])\r\n",
        "print(train_labels[0])\r\n",
        "print(train_masks[0])\r\n",
        "print(validation_inputs[0])\r\n",
        "print(validation_labels[0])\r\n",
        "print(validation_masks[0])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([   101,   9920,  59095,  12092,   8942, 118707,  35506,  25503,   9576,\n",
            "        119022,  11664,   9689, 118985,  10530,   8885,  18622,  62200,   9407,\n",
            "        119254,  68100,   9004,  32537,   9685,  11664,   9580,  37388,  18784,\n",
            "         19105,  10530,   9670,  89523,   9253,  10530,   9113,  11018,   9461,\n",
            "        118963,  11102,   8870,   8855,  77884,  48549,    102,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0])\n",
            "tensor(5., dtype=torch.float64)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "tensor([   101,   9283,  23990,  11102,   9367,  19855,  33797,   8942, 118707,\n",
            "         12453,   9685,  77884,  48549,  84097,   9920,  59095,  18589,  28911,\n",
            "          9580,  14867,   9074,  30873,  21614,   8895,  10739,   9249,  16985,\n",
            "         12424,   9487,  31720,   9511, 119147,  41850,   9565,  46216,  84097,\n",
            "         19709,   9083,  11018,   9487,  31720,   9511,  12508,  55698,  19767,\n",
            "          9238,  39420, 119297,  12092,   9553,  40311,   8946, 118708,  17594,\n",
            "          9058,  16985, 118748, 119185,  14040,  77884,  48549,   9751,  74322,\n",
            "         10892,   8924,  41521, 104467,   9056,   9429,  16758,  24683,   8870,\n",
            "          8855, 119081,  48345,   8928, 119122,  10892,   9678,  40032,   9379,\n",
            "        119089,  28578,   9596,   9638,   8843,  45465,  12030,  12508,   9638,\n",
            "         14523,  11287,  54780,   8843,  45465,  58303,  48345,    102,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0])\n",
            "tensor(5., dtype=torch.float64)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl1MiyabiZYq"
      },
      "source": [
        "# 배치 사이즈\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\r\n",
        "# 학습시 배치 사이즈 만큼 데이터를 가져옴\r\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\r\n",
        "train_sampler = RandomSampler(train_data)\r\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n",
        "\r\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\r\n",
        "validation_sampler = SequentialSampler(validation_data)\r\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBvpU-Hfgcth"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# **모델 생성**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wxzBUO-ikWd",
        "outputId": "826bc899-7abd-41fc-d7fa-7fd6414ecba2"
      },
      "source": [
        "# GPU 디바이스 이름 구함\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "\r\n",
        "# GPU 디바이스 이름 검사\r\n",
        "if device_name == '/device:GPU:0':\r\n",
        "    print('Found GPU at: {}'.format(device_name))\r\n",
        "else:\r\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clmBqrKlipGj",
        "outputId": "07295199-2df5-4426-f15a-a4a62850af60"
      },
      "source": [
        "# 디바이스 설정\r\n",
        "if torch.cuda.is_available():    \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "else:\r\n",
        "    device = torch.device(\"cpu\")\r\n",
        "    print('No GPU available, using the CPU instead.')"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY5e3b0eir_O",
        "outputId": "5ae6c03a-84c1-4e18-9b85-4c89fbf6aad0"
      },
      "source": [
        "# 분류를 위한 BERT 모델 생성\r\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=6)\r\n",
        "model.cuda()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knMPzKpxixZ8"
      },
      "source": [
        "# 옵티마이저 설정\r\n",
        "optimizer = AdamW(model.parameters(),\r\n",
        "                  lr = 2e-5, # 학습률\r\n",
        "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\r\n",
        "                )\r\n",
        "\r\n",
        "# 에폭수\r\n",
        "epochs = 4\r\n",
        "\r\n",
        "# 총 훈련 스텝 : 배치반복 횟수 * 에폭\r\n",
        "total_steps = len(train_dataloader) * epochs\r\n",
        "\r\n",
        "# 처음에 학습률을 조금씩 변화시키는 스케줄러 생성\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \r\n",
        "                                            num_warmup_steps = 0,\r\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSH9pxpjzC5r",
        "outputId": "46f2ab80-e6f2-40e8-b845-f305f50f0f7c"
      },
      "source": [
        "print(len(train_dataloader))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzCHV_ghj7DM"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# **모델 학습**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe78vCXHjDsg"
      },
      "source": [
        "# 정확도 계산 함수\r\n",
        "def flat_accuracy(preds, labels):\r\n",
        "    \r\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\r\n",
        "    labels_flat = labels.flatten()\r\n",
        "\r\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS43c4DxjGTu"
      },
      "source": [
        "# 시간 표시 함수\r\n",
        "def format_time(elapsed):\r\n",
        "\r\n",
        "    # 반올림\r\n",
        "    elapsed_rounded = int(round((elapsed)))\r\n",
        "    \r\n",
        "    # hh:mm:ss으로 형태 변경\r\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhOUmF83jIvI",
        "outputId": "b66dd86e-987b-48ff-8c75-24d05faf09ca"
      },
      "source": [
        "# 재현을 위해 랜덤시드 고정\r\n",
        "seed_val = 42\r\n",
        "random.seed(seed_val)\r\n",
        "np.random.seed(seed_val)\r\n",
        "torch.manual_seed(seed_val)\r\n",
        "torch.cuda.manual_seed_all(seed_val)\r\n",
        "\r\n",
        "# 그래디언트 초기화\r\n",
        "model.zero_grad()\r\n",
        "\r\n",
        "# 에폭만큼 반복\r\n",
        "for epoch_i in range(0, epochs):\r\n",
        "    \r\n",
        "    # ========================================\r\n",
        "    #               Training\r\n",
        "    # ========================================\r\n",
        "    \r\n",
        "    print(\"\")\r\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\r\n",
        "    print('Training...')\r\n",
        "\r\n",
        "    # 시작 시간 설정\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # 로스 초기화\r\n",
        "    total_loss = 0\r\n",
        "\r\n",
        "    # 훈련모드로 변경\r\n",
        "    model.train()\r\n",
        "        \r\n",
        "    # 데이터로더에서 배치만큼 반복하여 가져옴\r\n",
        "    for step, batch in enumerate(train_dataloader):\r\n",
        "        # 경과 정보 표시\r\n",
        "        if step % 500 == 0 and not step == 0:\r\n",
        "            elapsed = format_time(time.time() - t0)\r\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\r\n",
        "\r\n",
        "        # 배치를 GPU에 넣음\r\n",
        "        batch = tuple(t.to(device) for t in batch)\r\n",
        "        \r\n",
        "        # 배치에서 데이터 추출\r\n",
        "        b_input_ids, b_input_mask, b_labels = batch\r\n",
        "\r\n",
        "        \r\n",
        "\r\n",
        "        # Forward 수행                \r\n",
        "        outputs = model(b_input_ids, \r\n",
        "                        token_type_ids=None, \r\n",
        "                        attention_mask=b_input_mask, \r\n",
        "                        labels=b_labels.long())\r\n",
        "        \r\n",
        "        # 로스 구함\r\n",
        "        loss = outputs[0]\r\n",
        "\r\n",
        "        # 총 로스 계산\r\n",
        "        total_loss += loss.item()\r\n",
        "\r\n",
        "        # Backward 수행으로 그래디언트 계산\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # 그래디언트 클리핑\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "        # 그래디언트를 통해 가중치 파라미터 업데이트\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # 스케줄러로 학습률 감소\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "        # 그래디언트 초기화\r\n",
        "        model.zero_grad()\r\n",
        "\r\n",
        "    # 평균 로스 계산\r\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\r\n",
        "        \r\n",
        "    # ========================================\r\n",
        "    #               Validation\r\n",
        "    # ========================================\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"Running Validation...\")\r\n",
        "\r\n",
        "    #시작 시간 설정\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # 평가모드로 변경\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    # 변수 초기화\r\n",
        "    eval_loss, eval_accuracy = 0, 0\r\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\r\n",
        "\r\n",
        "    # 데이터로더에서 배치만큼 반복하여 가져옴\r\n",
        "    for batch in validation_dataloader:\r\n",
        "        # 배치를 GPU에 넣음\r\n",
        "        batch = tuple(t.to(device) for t in batch)\r\n",
        "        \r\n",
        "        # 배치에서 데이터 추출\r\n",
        "        b_input_ids, b_input_mask, b_labels = batch\r\n",
        "        \r\n",
        "        # 그래디언트 계산 안함\r\n",
        "        with torch.no_grad():     \r\n",
        "            # Forward 수행\r\n",
        "            outputs = model(b_input_ids, \r\n",
        "                            token_type_ids=None, \r\n",
        "                            attention_mask=b_input_mask)\r\n",
        "        \r\n",
        "        # 로스 구함\r\n",
        "        logits = outputs[0]\r\n",
        "\r\n",
        "        # CPU로 데이터 이동\r\n",
        "        logits = logits.detach().cpu().numpy()\r\n",
        "        label_ids = b_labels.to('cpu').numpy()\r\n",
        "        \r\n",
        "        # 출력 로짓과 라벨을 비교하여 정확도 계산\r\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\r\n",
        "        eval_accuracy += tmp_eval_accuracy\r\n",
        "        nb_eval_steps += 1\r\n",
        "\r\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\r\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\r\n",
        "\r\n",
        "print(\"\")\r\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch   500  of    501.    Elapsed: 0:06:10.\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epcoh took: 0:06:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.76\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch   500  of    501.    Elapsed: 0:06:09.\n",
            "\n",
            "  Average training loss: 0.53\n",
            "  Training epcoh took: 0:06:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch   500  of    501.    Elapsed: 0:06:09.\n",
            "\n",
            "  Average training loss: 0.45\n",
            "  Training epcoh took: 0:06:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch   500  of    501.    Elapsed: 0:06:10.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epcoh took: 0:06:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7SzL1IBe1Dm"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# **새로운 문장 테스트**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz-LXWOd0BSx"
      },
      "source": [
        "# 입력 데이터 변환\r\n",
        "def convert_input_data(sentences):\r\n",
        "\r\n",
        "    # BERT의 토크나이저로 문장을 토큰으로 분리\r\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\r\n",
        "\r\n",
        "    # 입력 토큰의 최대 시퀀스 길이\r\n",
        "    MAX_LEN = 128\r\n",
        "\r\n",
        "    # 토큰을 숫자 인덱스로 변환\r\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\r\n",
        "    \r\n",
        "    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\r\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\r\n",
        "\r\n",
        "    # 어텐션 마스크 초기화\r\n",
        "    attention_masks = []\r\n",
        "\r\n",
        "    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\r\n",
        "    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\r\n",
        "    for seq in input_ids:\r\n",
        "        seq_mask = [float(i>0) for i in seq]\r\n",
        "        attention_masks.append(seq_mask)\r\n",
        "\r\n",
        "    # 데이터를 파이토치의 텐서로 변환\r\n",
        "    inputs = torch.tensor(input_ids)\r\n",
        "    masks = torch.tensor(attention_masks)\r\n",
        "\r\n",
        "    return inputs, masks"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKAYbBDV0Gg-"
      },
      "source": [
        "# 문장 테스트\r\n",
        "def test_sentences(sentences):\r\n",
        "\r\n",
        "    # 평가모드로 변경\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    # 문장을 입력 데이터로 변환\r\n",
        "    inputs, masks = convert_input_data(sentences)\r\n",
        "\r\n",
        "    # 데이터를 GPU에 넣음\r\n",
        "    b_input_ids = inputs.to(device)\r\n",
        "    b_input_mask = masks.to(device)\r\n",
        "            \r\n",
        "    # 그래디언트 계산 안함\r\n",
        "    with torch.no_grad():     \r\n",
        "        # Forward 수행\r\n",
        "        outputs = model(b_input_ids, \r\n",
        "                        token_type_ids=None, \r\n",
        "                        attention_mask=b_input_mask)\r\n",
        "\r\n",
        "    # 로스 구함\r\n",
        "    logits = outputs[0]\r\n",
        "\r\n",
        "    # CPU로 데이터 이동\r\n",
        "    logits = logits.detach().cpu().numpy()\r\n",
        "\r\n",
        "    return logits"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtR_cQEz2FeA",
        "outputId": "d9d649d5-5347-4101-a785-6579a95f1fa1"
      },
      "source": [
        "logits = test_sentences(['전자레인지 없음 벌레 있음 인원 추가비에 침구류 포함이라고 했는데 침구류 인원보다 적게 제공받음 최대인 원명인 곳에 화장실 개 이런 부분은 안내되어 있던 부분도 있고 자연 속에 있는 곳이니까 침구류는 더 달라고 했으며 주셨겠죠하며 이해할 수 있었습니다 하지만 마지막 날 인원 추가 숯 그릴비를 받는 부분에서 결제를 후 결제 확인표시를 하는 것을 보고 있는데 잠시 후같이 간 일행과 통화하다 아 그 추가비 결제했다는 이야기를 들었습니다 중복으로 받은 거죠 그래서 전화를 한 후 사정을 이야기하니 계좌번호를 알려주면 입금해준다고 하여 문자로 알렸는데 거기서 오는 답 두 분께서 서로 의사소통의 문제가 있어 결제를 또 하신 거네요 네 어찌 추가 금 결제하는 것이 돈을 받는 분께서 돈을 받았고 안 받았고를 하신 후 요구하거나 안내했어야 하는 건 아닌지요 원만하게 해결돼서 됐다 싶었는데 마지막 저 문자는 도대체 너희가 서로 냈는지 모르고 돈 더 낸 건데 왜 그려냐 라는 식으로 들리는 건 뭘까요 동행과 이야기하기로 제 쪽에서 추가요금을 내기로 했던 부분인데 저희가 먼저 출발하고 조금 있다 동행분이 안내소에 방미 전달하면서 혹시나 한 번 확인 한 것뿐인데 과연 이것이 저런 식의 대답으로 돌아오는 것인지 실수로 중복으로 받은 것인데 돈을 왜 서로 낸 거 몰랐냐는 식인 건지 네가 안 냈냐 네가 냈냐 이런 걸 물으면서까지 그것을 확인했어야 했는지 또한 만약 통화하다 이 부분이 얘기 안되었으면 이 숙박업소 측은 아 손님이 잘못 낸 건데 뭐 하고 지나갔을 일인지 기분 좋아 여행하고 돌아오는 길 예상치도 못한 곳에서의 문자로 당황스러웠네요 이런 글은 여기 남기는 이유는 그 연락을 담당하신 분이 사장님이 아닌지 직원분이신지 모르지만 홈페이지에도 이런 글을 남기는 곳 없고 홈페이지 바로 전화연결하는 번호는 그 담당자분 이받았고 고민하다 이런 불쾌했던 감정을 알려야 할 것 같아 글을 남깁니다'])\r\n",
        "\r\n",
        "print(logits)\r\n",
        "print(np.argmax(logits))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.6671095   4.633141    0.76591396 -0.3764771  -1.5845273  -0.75470215]]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QajAVm2s2aHU",
        "outputId": "cf2a289d-8d1a-48b7-c092-08e577336f1f"
      },
      "source": [
        "logits = test_sentences(['코로나 기간 중 여행이라 체인 호텔의 안정성을 믿고 예약을 했습니다 호텔 자체의 시설이나 체크인 객실 컨디션 등에 대해서는 큰 불만이 없었으나 체크아웃 당일 오전에 침대에서 벌레를 확인했습니다 흔한 날파리류의 벌레가 아니라 물어서 알레르기를 일으킬 수 있는 종류의 벌레였습니다 부모님께서 체크아웃을 하시는 거라 저는 예약만 리셉션 직원에게 항의했지만 돌아오는 답변은 친환경 호텔이라 어쩔 수 없다 였습니다 아무리 친환경 호텔이고 아무리 장마 기간이라지만 호텔 침구에 벌레라니요 별거 아닌 벌레라 생각하실지 몰라 사진 첨부합니다 코로나 상황에 방역을 열심히 하고 계신다는데 전화 통화 완료 방역을 열심히 하신다면 더더군다나 이런 벌레는 없어야 하는 게 맞는 것 같네요 불쾌한 상황에 대한 사과는 없고 다음 분들께도 또 친환경 호텔이라 어쩔 수 없다고 하시겠죠 앞으로는 호텔 설명 시에 친환경 호텔이라 객실에서 벌레를 발견하실 수 있습니다 그렇지만 호텔은 책임지지 않는다는 내용까지 명시되어야 할 것 같습니다 아기 데리고 가시는 분들은 특히 조심하시기 바랍니다'])\r\n",
        "\r\n",
        "print(logits)\r\n",
        "print(np.argmax(logits))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-2.4617867   2.9480758   1.894509    0.6768581  -0.67458725 -1.3741947 ]]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY7QS9PZ227C",
        "outputId": "7b0d44f4-27f2-438a-f9ec-953e8b8f2739"
      },
      "source": [
        "logits = test_sentences(['한화리조트 한화 계열은 피닉스 평창 중앙일보 계열 이랑 다른 법인이다 잘못 찾아가는 일이 없도록 하자 레드동 핑크 등은 한화리조트 평창 나머지는 피닉스 평창이다 일단 주차장이 좁다 괜히 실내주차하지 말고 아예 지하층 야외주차장이 있으니 거기하면 상대적으로 여유 있게 주차할 수 있다 블루 케니언에서 너무 떨어져 있고 곧바로 연결되지 않는다 반면 피닉스 블루종은 지하층에서 센터 플라자 블루 케니언으로 연결되는 통로가 있다 차를 몰아 블루 케니언 쪽으로 이동하는 게 낫다 걷는 거리가 상당하다 블루 케니언 옆 건물이 센터 플라자다 층에 파리바게뜨가 있는 곳 괜히 비발디파크나 롯데리조트 생각하고 피닉스 블루 등 지하로 가지 말기 바란다 객실은 오래된 느낌이 확 난다 key도 아직 철제 key를 쓴다 마룻바닥 디자인은 년 전 올드 한 감성으로 색깔이 진한 갈색이나 도배나 이불장 걸레받이는 연한 나무 무늬로 최근에 바꾼 것으로 추정되어 선명한 대비를 이룬다 이왕이면 바닥재까지 바꿨으면 깨끗한 느낌이 들었을 텐데 아직 멀쩡해서 바닥재는 안 바꿨나 보다 하지만 객실은 아주 넓다 설악 쏘라노보다 넓고 전반적으로 조용하다 설악 쏘라노는 객실 앞에 식당에서 노래 틀고 코로나 이전에는 야간에 놀이기구에서 불빛 나고 해서 시끄러워 밤에 자기가 힘들었다 다만 액티비티 차원에서는 같은 한화 계열 중에서는 워터피아랑 산책로가 있는 설악 쏘라노가 나은 것 같다 이러저러한 불편사항을 생각하면 중앙에서 마저 인수해서 부지를 합치고 공사를 해서 확장하는 편이 나아 보인다 블루 케니언 도 더 크게 만들고 이 한화리조트 평창은 산책코스가 있는 것도 아니고 달랑 개 동으로 부대시설이 너무 열악하다 이 상태로는 오래 버티기엔 힘들어 보인다 인기가 많아 보이지도 않는다 대명 홍천 비발디파크랑 비교했을 때 많이 열악하다 이점은 휘닉스파크 센터 플라자도 마찬가지 콘도는 역시 대명'])\r\n",
        "\r\n",
        "print(logits)\r\n",
        "print(np.argmax(logits))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.4030697   2.7196681   2.0125906   0.94277585 -0.93514234 -1.962033  ]]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msVtAn_O0Jrf",
        "outputId": "b8f5eb4f-9c1e-414d-9e43-6ee612100081"
      },
      "source": [
        "logits = test_sentences(['호텔 생각하면 실망하실 거예요 괜찮은 모텔 정도로 보면 적당합니다'])\r\n",
        "\r\n",
        "print(logits)\r\n",
        "print(np.argmax(logits))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-3.3275385  -0.03545406  0.5635631   1.4716722   0.9876551   0.31368235]]\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-TRxzhy2cYS",
        "outputId": "bfbe823f-cb87-4369-8651-8723607491b1"
      },
      "source": [
        "logits = test_sentences(['콘도형 호텔형 두 군데 있는데 콘도형에 묵었습니다 박 취사 유무 차이 입장료는 일반인들에게 받고 투숙객은 안 받더라고요 ㅎ 콘도형 기준으로 알려드립니다 장점 날씨 좋은 날에는 뷰가 좋음 바다가 홀수방은 부분 오션 뷰 주변에 파릇한 공원 산책할 수 있음 편의시설이 좋음 작은 마트 노래방 헬스센터 입장할 때 분위기 좋음 단점 홀수방 선택 시 밤에 밖에서 객실이 확실하게 보임 커튼 필 호텔 직원들이 잘 안 웃음 콘도형은 물 비치가 안 되어있음 밤에 벌레가 많아서 문을 닫아야 함 에어컨이 좀 약해서 하루 종일 틀어도 방 전체에 그렇게 시원하지 않음 에어컨 밑은 시원합니다 ㅋㅋ 호텔 콘도 밖에는 뭐가 없어요 호텔 안에서 할 수 있긴 하지만 비치된 물품은 드라이기 샴푸 보디워시 비누 일용 수건 에프킬라 전자레인지 전기밥솥 인 기준 식기세트 냄비 종 도마 등침대 베개는 그럭저럭 푹신합니다 룸서비스로 콤비네이션 피자와 양념치킨 주문했는데요 콤비네이션은 가격에 비해 양도 질도 부족합니다 양념치킨은 옛날 치킨 맛 개인 취향 저격 조식 현재 투숙객 이벤트 할인해서 할인 가격으론 괜찮았습니다 스카이라운지 그리고 호텔형 리조트도 구경해보고 싶었는데 짧은 시간에 구경하기가 힘들었어요 개인적으로 피곤하기도 하고 공원으로 가서 스카이워크가 있었는데요 거의 급 경사 수준의 장소에 계단이 설치되어있어서 고소공포증 있는 분은 가는 길에 포기할 것 같아요 제가 그랬어요 스카이워크는 인만 들어갈 수 있습니다 순서 지키면서 차례대로 공포증 없으신 분에게는 정말 장관인 곳 여유 있으시다면 주변 뷰도 보시고 편하게 즐기시면 좋을 것 같아요 호텔형은 어떤지 모르지만 콘도형보다 좋겠죠 주변 경관으로 인해 박 더 해보고 싶은 생각이 들었어요'])\r\n",
        "\r\n",
        "print(logits)\r\n",
        "print(np.argmax(logits))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.3107493  -2.3357615  -1.037825    0.88406044  2.4160337   0.33331418]]\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1pZe1oe2czk",
        "outputId": "ed8c7c1a-6dc5-49e5-8b6d-3a5a8091b3af"
      },
      "source": [
        "logits = test_sentences(['흠 적을게 많아서 요약하자면 인터넷상에 있는 사진은 굉장히 허름해 보이지만 실제로 가면 불편한 것 허름한 느낌 전혀 들지 않습니다 그리고 엄 청 깨끗합니다 그리고 놀란 게 수건 직접 빨래하시는 거 같은데 냄새 하나도 안 나요 친절한 사장님 사장님 정말 친절하세요 오히려 부담될 정도로 잘 챙겨주시고 좋습니다 요즘 펜션 다른 곳 가보면 눈치 많이 보이고 지켜달라는 것 많은데 이곳은 손님을 편하게 쉬게 해야겠다는 느낌을 받았습니다 가격제가 양구에서 원래 숙박하려고 했다가 예약이 다 차서 화천까지 갔는데 평균 숙박료가 일대에 모두 만 만입니다 하지만 여긴 만 천 원 바비큐도 인 기준 만원 저렴한데 깨끗해서 좋습니다 총평 가격이 싼데 허름한 사진 때문에 고민된다면 바로 예약하세요 깨끗하고 저렴하고 친절하십니다 단 수압이 쪼금 약합니다'])\r\n",
        "\r\n",
        "print(logits)\r\n",
        "print(np.argmax(logits))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-2.0824466 -1.9720962 -2.186492  -1.5564895  1.2332951  4.799001 ]]\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}